{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/lisaanne/lib/coco-caption/')\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "import numpy as np\n",
    "from bias_detection import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json file for annotations for entire set\n",
    "save_file = '/data/lisaanne/fairness/checkpoints/LW_train/result-checkpoint-1000000.json'   \n",
    "predicted_captions = json.load(open(save_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.576215\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.13s)\n",
      "creating index...\n",
      "index created!\n",
      "Evaluation over the entire MSCOCO set:\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'reflen': 381597, 'guess': [383026, 342522, 302018, 261514], 'testlen': 383026, 'correct': [254819, 121042, 55258, 26936]}\n",
      "ratio: 1.00374478835\n",
      "Bleu_1: 0.665\n",
      "Bleu_2: 0.485\n",
      "Bleu_3: 0.350\n",
      "Bleu_4: 0.258\n",
      "computing METEOR score...\n",
      "METEOR: 0.228\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.492\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.791\n"
     ]
    }
   ],
   "source": [
    "coco = COCO('coco/annotations/captions_val2014.json')\n",
    "generation_coco = coco.loadRes(save_file)\n",
    "coco_evaluator = COCOEvalCap(coco, generation_coco)\n",
    "print \"Evaluation over the entire MSCOCO set:\"\n",
    "coco_evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over the biased set:\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "Evaluation over the biased MSCOCO set:\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'reflen': 104406, 'guess': [104419, 93496, 82573, 71650], 'testlen': 104419, 'correct': [73957, 38033, 17474, 9229]}\n",
      "ratio: 1.00012451392\n",
      "Bleu_1: 0.708\n",
      "Bleu_2: 0.537\n",
      "Bleu_3: 0.394\n",
      "Bleu_4: 0.298\n",
      "computing METEOR score...\n",
      "METEOR: 0.255\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.530\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.792\n"
     ]
    }
   ],
   "source": [
    "print \"Evaluation over the biased set:\"\n",
    "\n",
    "#First create json with unbiased data.  \n",
    "#Kaylee -- you might want to update this with the script you wrote; I made a text file with all the image names\n",
    "#since this seemed like something I would use a lot (used your script to make this txt file)\n",
    "\n",
    "bias_ids = open('/data/lisaanne/fairness/data/img_names.txt').readlines()\n",
    "bias_ids = [int(id) for id in bias_ids]\n",
    "\n",
    "bias_captions = []\n",
    "for caption in predicted_captions:\n",
    "    if caption['image_id'] in bias_ids:\n",
    "        bias_captions.append(caption)\n",
    "        \n",
    "bias_save_file = save_file.replace('.json', '_bias.json')\n",
    "with open(bias_save_file, 'w') as outfile:\n",
    "    json.dump(bias_captions, outfile)\n",
    "\n",
    "generation_coco = coco.loadRes(bias_save_file)\n",
    "coco_evaluator = COCOEvalCap(coco, generation_coco)\n",
    "\n",
    "# evaluate on a subset of images by setting\n",
    "# cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "# please remove this line when evaluating the full validation set\n",
    "coco_evaluator.params['image_id'] = generation_coco.getImgIds()\n",
    "\n",
    "print \"Evaluation over the biased MSCOCO set:\"\n",
    "coco_evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu_3: 0.303824\n",
      "Bleu_4: 0.188967\n",
      "METEOR: 0.271968\n",
      "CIDEr: 0.791515\n"
     ]
    }
   ],
   "source": [
    "bias_ids = [int(id) for id in bias_ids]\n",
    "bias_ids = set(bias_ids) & set(coco_evaluator.imgToEval.keys())\n",
    "bleu_3 = np.mean([coco_evaluator.imgToEval[id][\"Bleu_3\"] for id in bias_ids])\n",
    "bleu_4 = np.mean([coco_evaluator.imgToEval[id][\"Bleu_4\"] for id in bias_ids])\n",
    "cider = np.mean([coco_evaluator.imgToEval[id][\"CIDEr\"] for id in bias_ids])\n",
    "meteor = np.mean([coco_evaluator.imgToEval[id][\"METEOR\"] for id in bias_ids])\n",
    "\n",
    "print \"Bleu_3: %f\" %bleu_3\n",
    "print \"Bleu_4: %f\" %bleu_4\n",
    "print \"METEOR: %f\" %meteor\n",
    "print \"CIDEr: %f\" %cider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get F1 scores for man/woman + some eval preprocessing\n",
    "#From Kaylee's code\n",
    "\n",
    "img_2_anno_dict = create_dict_from_list(pickle.load(open(target_train)))\n",
    "img_2_anno_dict.update(create_dict_from_list(pickle.load(open(target_test))))\n",
    "img_2_anno_dict.update(create_dict_from_list(pickle.load(open(target_val))))\n",
    "\n",
    "img_2_anno_dict_simple = {}\n",
    "for key, value in img_2_anno_dict.iteritems():\n",
    "    id = int(key.split('_')[-1].split('.jpg')[0])\n",
    "    img_2_anno_dict_simple[id] = {}\n",
    "    img_2_anno_dict_simple[id]['male'] = value[0]\n",
    "    img_2_anno_dict_simple[id]['female'] = value[1]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predicted):\n",
    "    f_tp = 0.\n",
    "    f_fp = 0.\n",
    "    f_tn = 0.\n",
    "    f_other = 0.\n",
    "    f_total = 0.\n",
    "    \n",
    "    \n",
    "    m_tp = 0.\n",
    "    m_fp = 0.\n",
    "    m_tn = 0.\n",
    "    m_other = 0.\n",
    "    m_total = 0.\n",
    "\n",
    "    for prediction in predicted:\n",
    "        image_id = prediction['image_id']\n",
    "        male = img_2_anno_dict_simple[image_id]['male']\n",
    "        female = img_2_anno_dict_simple[image_id]['female']\n",
    "        pred_male = classified_as_man(prediction['caption'].split(' '))\n",
    "        pred_female = classified_as_woman(prediction['caption'].split(' '))\n",
    "        if (female & pred_female):\n",
    "            f_tp += 1\n",
    "        if (male & pred_male):\n",
    "            m_tp += 1\n",
    "        if (male & pred_female):\n",
    "            f_fp += 1\n",
    "        if (female & pred_male):\n",
    "            m_fp += 1\n",
    "        if ((not female) & (not pred_female)):\n",
    "            f_tn += 1\n",
    "        if ((not male) & (not pred_male)):\n",
    "            m_tn += 1\n",
    "        pred_other = (not pred_male) & (not pred_female)\n",
    "        if (female & pred_other):\n",
    "            f_other += 1\n",
    "        if (male & pred_other):\n",
    "            m_other += 1\n",
    "        if female:\n",
    "            f_total += 1\n",
    "        if male:\n",
    "            m_total += 1\n",
    "\n",
    "        \n",
    "    #precision_f = (f_tp)/(f_tp+f_fp)\n",
    "    #recall_f = (f_tp)/(f_tp+f_n)\n",
    "    \n",
    "    print \"Of female images:\"\n",
    "    print \"Woman predicted %f percent.\" %(f_tp/f_total)\n",
    "    print \"Man predicted %f percent.\" %(m_fp/f_total)\n",
    "    print \"Other predicted %f percent.\" %(f_other/f_total)\n",
    "    \n",
    "    print \"Of male images:\"\n",
    "    print \"Man predicted %f percent.\" %(m_tp/m_total)\n",
    "    print \"Woman predicted %f percent.\" %(f_fp/m_total)\n",
    "    print \"Other predicted %f percent.\" %(m_other/m_total)\n",
    "    #precision_m = (m_tp)/(m_tp+m_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_all = json.load(open(save_file))\n",
    "predicted = [p for p in predicted_all if p['image_id'] in bias_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of female images:\n",
      "Woman predicted 0.457487 percent.\n",
      "Man predicted 0.335660 percent.\n",
      "Other predicted 0.206853 percent.\n",
      "Of male images:\n",
      "Man predicted 0.747652 percent.\n",
      "Woman predicted 0.074379 percent.\n",
      "Other predicted 0.177969 percent.\n"
     ]
    }
   ],
   "source": [
    "accuracy(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79101707009731126"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([coco_evaluator.imgToEval[id][\"CIDEr\"] for id in coco_evaluator.imgToEval.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
